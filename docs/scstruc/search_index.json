[["index.html", "scstruc: causal assessment of gene regulatory network using Bayesian network Chapter 1 Introduction 1.1 Installation and prerequiresties", " scstruc: causal assessment of gene regulatory network using Bayesian network Noriaki Sato 2025-04-06 Chapter 1 Introduction scstruc is a package designed to estimate gene regulatory networks (GRNs) from single-cell transcriptomics (SCT) data using Bayesian networks (BN). Notably, the package focuses on evaluating the inferred networks based on their causal validity. Also, it enables probabilistic reasoning and the extraction of differences in regulatory relationships between groups, as well as the comprehensive visualization for understanding the resulting GRNs. BN estimate directed relationships between genes and are useful for GRN inference. The package supports various algorithms for handling the unique characteristics of the SCT data, such as zero-inflation of each cell’s gene expression. 1.1 Installation and prerequiresties devtools::install_github(&quot;noriakis/scstruc&quot;) We need some packages that needs to be installed before using the full functions of scstruc. CCDr algorithm from the following repository: devtools::install_github(&quot;itsrainingdata/sparsebnUtils&quot;) devtools::install_github(&quot;noriakis/ccdrAlgorithm&quot;) HurdleNormal package from the following repository: devtools::install_github(&quot;amcdavid/HurdleNormal&quot;) For CCDr algorithm and HurdleNormal package, please consult the original papers (McDavid et al. 2019; Aragam and Zhou 2015). References Aragam, Bryon, and Qing Zhou. 2015. “Concave Penalized Estimation of Sparse Gaussian Bayesian Networks.” Journal of Machine Learning Research 16 (69): 2273–2328. http://jmlr.org/papers/v16/aragam15a.html. McDavid, Andrew, Raphael Gottardo, Noah Simon, and Mathias Drton. 2019. “GRAPHICAL MODELS FOR ZERO-INFLATED SINGLE CELL GENE EXPRESSION.” The Annals of Applied Statistics 13 (2): 848–73. https://doi.org/10.1214/18-AOAS1213. "],["basic-usage-of-scstruc.html", "Chapter 2 Basic usage of scstruc 2.1 Bayesian network 2.2 Preprocessing of single-cell transcriptomics data and preparation 2.3 Actual structure learning", " Chapter 2 Basic usage of scstruc This part will present how to use the package for the inference of the network and evaluation. Basically, if you have the object storing (normalized) expression matrix, the primary function scstruc can perform inference. We will use the mock data generated by scran::mockSCE function here, and the usage of actual data obtained from experimental samples are introduced in (actual?). library(scran) library(scstruc) library(ggraph) library(bnlearn) library(tibble) set.seed(0) sce &lt;- mockSCE() sce &lt;- logNormCounts(sce) sce #&gt; class: SingleCellExperiment #&gt; dim: 2000 200 #&gt; metadata(0): #&gt; assays(2): counts logcounts #&gt; rownames(2000): Gene_0001 Gene_0002 ... Gene_1999 #&gt; Gene_2000 #&gt; rowData names(0): #&gt; colnames(200): Cell_001 Cell_002 ... Cell_199 #&gt; Cell_200 #&gt; colData names(4): Mutation_Status Cell_Cycle #&gt; Treatment sizeFactor #&gt; reducedDimNames(0): #&gt; mainExpName: NULL #&gt; altExpNames(1): Spikes We obtain simulated data by mockSCE function in scuttle, and log-normalize the count with the default parameter. 2.1 Bayesian network Bayesian networks are probabilistic graphical models that represent a set of random variables \\(X = \\{X_1, \\dots, X_N\\}\\), and their conditional dependencies using a directed acyclic graph (DAG). They are widely used in various domains, such as economics, medicine, decision-making, and for modeling complex relationships among variables such as transcriptomics data. In the package, we infer the relationships among genes, representing the transcriptional regulation between them. The graphical separation between nodes in G implies the conditional independence of the corresponding variables and leads to factorization: \\[ \\text{logit}\\left(\\Pr(Z_{ij} = 1)\\right) \\sim \\beta_i G_j, \\] \\[ \\Pr(Y_{ij} = y \\mid Z_{ij} = 1) \\sim N(\\beta_i G_j, \\sigma^2). \\] 2.2 Preprocessing of single-cell transcriptomics data and preparation To reduce computational burden and to obtain interpretable results, some preprocessing steps and the preparation is suggested. 2.2.1 Calculating metacell expression The package can use metacell approach to reduce the computational burden required to perform structure learning from SCT data. The package features superCellMat function using SuperCell package available in CRAN. It employs a cell aggregation approach to reduce dataset complexity while preserving key biological variability (Bilous et al. 2022). This step is optional and one can use their untransformed data directly. library(SuperCell) dim(sce) #&gt; [1] 2000 200 rowData(sce)[&quot;ID&quot;] &lt;- row.names(sce) sce &lt;- superCellMat(sce, ID=&quot;ID&quot;) #&gt; 2000 20 dim(sce) #&gt; [1] 2000 20 2.2.2 Obtaining interesting gene identifiers For inference, we first subset to genes in the interesting biological pathway (such as ECM receptor interaction in the cells annotated as vascular endothelial cells). For this purpose, two functions are prepared. One is fetching gene identifiers from Gene Ontology, and another is from KEGG PATHWAY. 2.2.2.1 getGOGenes We can obtain genes involved in gene ontology by using the identifier. library(scstruc) library(org.Mm.eg.db) genes &lt;- getGOGenes(&quot;GO:0030198&quot;, orgDb=org.Mm.eg.db) genes[1:5] #&gt; [1] &quot;Abl1&quot; &quot;Abl1&quot; &quot;Abl2&quot; &quot;Adamts1&quot; &quot;Adamts1&quot; 2.2.2.2 getKEGGPathwayGenes We can obtain genes involved in KEGG PATHWAY by using the KEGG identifier with organism identifier. The below is obtaining genes involved in mTOR signaling pathway in Mus musculus. You need to specify organism database in order to correcrly convert the obtained identifiers. library(KEGGREST) genes &lt;- getKEGGPathwayGenes(&quot;mmu04150&quot;, orgDb=org.Mm.eg.db) genes[1:5] #&gt; [1] &quot;Prkaa1&quot; &quot;Prkaa2&quot; &quot;Atp6v1h&quot; &quot;Prr5&quot; &quot;Braf&quot; You can pass these genes for the inference by scstruc function. Also, we published a package that estimates GRNs from omics data based on enrichment analysis results in the past, and the approach could be useful (Sato et al. 2022). 2.3 Actual structure learning scstruc function needs SingleCellExperiment object or those storing gene expression data. Also, gene list must be specified. In this analysis, randomly subset gene identifiers are used as input. changeSymbol argument here refers to the changing of the gene names of subset matrix to those specified in symbolColumn argument (default to Symbol). In this example, we do not have corresponding symbol and the argument is set to FALSE. Also, algorithm is set to mmhc by default, which performs max-min hill climbing approach. included_genes &lt;- sample(row.names(sce), 100) gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE) names(gs) #&gt; [1] &quot;net&quot; &quot;data&quot; gs$net #&gt; #&gt; Bayesian network learned via Hybrid methods #&gt; #&gt; model: #&gt; [Gene_0019][Gene_0061][Gene_0097][Gene_0099][Gene_0101] #&gt; [Gene_0108][Gene_0123][Gene_0198][Gene_0224][Gene_0239] #&gt; [Gene_0242][Gene_0290][Gene_0292][Gene_0322][Gene_0325] #&gt; [Gene_0332][Gene_0352][Gene_0354][Gene_0374][Gene_0418] #&gt; [Gene_0433][Gene_0451][Gene_0630][Gene_0642][Gene_0676] #&gt; [Gene_0744][Gene_0750][Gene_0795][Gene_0831][Gene_0900] #&gt; [Gene_1018][Gene_1024][Gene_1030][Gene_1048][Gene_1124] #&gt; [Gene_1144][Gene_1178][Gene_1180][Gene_1241][Gene_1273] #&gt; [Gene_1283][Gene_1316][Gene_1342][Gene_1512][Gene_1646] #&gt; [Gene_1672][Gene_1795][Gene_1889][Gene_1976] #&gt; [Gene_0238|Gene_0019:Gene_0795][Gene_0466|Gene_0224] #&gt; [Gene_0558|Gene_0322][Gene_0580|Gene_0332] #&gt; [Gene_0711|Gene_0433:Gene_0795:Gene_1273] #&gt; [Gene_0716|Gene_0676][Gene_0772|Gene_0354] #&gt; [Gene_0840|Gene_0642][Gene_0859|Gene_0744] #&gt; [Gene_0954|Gene_0642][Gene_0979|Gene_0325] #&gt; [Gene_0980|Gene_0433][Gene_1059|Gene_0290] #&gt; [Gene_1084|Gene_0101][Gene_1132|Gene_1030] #&gt; [Gene_1173|Gene_0097:Gene_1512][Gene_1310|Gene_0292] #&gt; [Gene_1360|Gene_1024][Gene_1372|Gene_1048] #&gt; [Gene_1384|Gene_0433][Gene_1483|Gene_1018:Gene_1273] #&gt; [Gene_1487|Gene_1030:Gene_1672][Gene_1509|Gene_0292] #&gt; [Gene_1542|Gene_1342][Gene_1642|Gene_1178:Gene_1241] #&gt; [Gene_1677|Gene_1180:Gene_1889][Gene_1739|Gene_0676] #&gt; [Gene_1751|Gene_1646][Gene_1775|Gene_1316] #&gt; [Gene_1855|Gene_0108][Gene_1859|Gene_0831:Gene_1144] #&gt; [Gene_1863|Gene_0630][Gene_1982|Gene_0061] #&gt; [Gene_0427|Gene_0108:Gene_1059][Gene_0792|Gene_1360] #&gt; [Gene_1257|Gene_1863][Gene_1262|Gene_0900:Gene_1855] #&gt; [Gene_1518|Gene_0451:Gene_1982][Gene_1519|Gene_1084] #&gt; [Gene_1653|Gene_0772][Gene_1804|Gene_1775] #&gt; [Gene_0048|Gene_0198:Gene_1804][Gene_0617|Gene_1257] #&gt; [Gene_0786|Gene_0427][Gene_1159|Gene_0792] #&gt; [Gene_1742|Gene_0792][Gene_0398|Gene_0786] #&gt; [Gene_0463|Gene_0786:Gene_1342] #&gt; [Gene_0490|Gene_0352:Gene_1159:Gene_1283] #&gt; [Gene_1140|Gene_1742][Gene_1991|Gene_1140] #&gt; nodes: 100 #&gt; arcs: 67 #&gt; undirected arcs: 0 #&gt; directed arcs: 67 #&gt; average markov blanket size: 1.70 #&gt; average neighbourhood size: 1.34 #&gt; average branching factor: 0.67 #&gt; #&gt; learning algorithm: #&gt; Max-Min Hill-Climbing #&gt; constraint-based method: #&gt; Max-Min Parent Children #&gt; conditional independence test: #&gt; Pearson&#39;s Correlation #&gt; score-based method: Hill-Climbing #&gt; score: BIC (Gauss.) #&gt; alpha threshold: 0.05 #&gt; penalization coefficient: 1.497866 #&gt; tests used in the learning procedure: 22042 #&gt; optimized: TRUE By default, it returns the bn object from bnlearn and the actual data used for the inference. If returnBn is set to FALSE, the function returns the tidygraph object. These can be used for various downstream tasks, such as visualization, probabilistic reasoning, evaluation based on biological pathway information. The details including the use of various algorithms for learning is described in the following sections. References Bilous, Mariia, Loc Tran, Chiara Cianciaruso, Aurélie Gabriel, Hugo Michel, Santiago J Carmona, Mikael J Pittet, and David Gfeller. 2022. “Metacells Untangle Large and Complex Single-Cell Transcriptome Networks.” BMC Bioinformatics 23 (1): 336. https://doi.org/10.1186/s12859-022-04861-1. Sato, Noriaki, Yoshinori Tamada, Guangchuang Yu, and Yasushi Okuno. 2022. “CBNplot: Bayesian Network Plots for Enrichment Analysis.” Bioinformatics (Oxford, England) 38 (10): 2959–60. https://doi.org/10.1093/bioinformatics/btac175. "],["penalized-regressions.html", "Chapter 3 Penalized regressions 3.1 Algorithms", " Chapter 3 Penalized regressions Penalized regression is a type of regression analysis that includes a penalty term to prevent overfitting and improve the generalizability of the model. It is particularly useful when dealing with high-dimensional data where the number of predictors is large compared to the number of observations. In terms of inference of GRN and BN structure learning, the penalized regression has been used successfully (Schmidt, Niculescu-Mizil, and Murphy 2007). The package implments the core algorithms for the use of penalization in the BN structure learning described below. 3.1 Algorithms 3.1.1 glmnet_BIC and glmnet_CV For L1 regularization (LASSO), the package uses the popular R library glmnet. The glmnet_BIC specified in the argument algo will select the lambda by the minimum BIC criteria, while the glmnet_CV choose the lambda based on cross validation with the specified fold numbers. glmnetBICpath returns the BIC path given the data.frame and the node name to be modeled. plot contains the BIC path plot, and fit contains the fitted object and BIC the lambda and BIC data.frame. The maximize function can be chosen by maximize argument in algorithm.args list. Greedy Equivalence Search can be performed via setting this argument to ges. library(scran) library(scstruc) library(bnlearn) sce &lt;- mockSCE() sce &lt;- logNormCounts(sce) included_genes &lt;- sample(row.names(sce), 30) ## Inference based on glmnet_CV gs &lt;- scstruc(sce, included_genes, algorithm=&quot;glmnet_CV&quot;, changeSymbol=FALSE) gs$net #&gt; #&gt; Bayesian network learned via Score-based methods #&gt; #&gt; model: #&gt; [Gene_0048][Gene_0165][Gene_0209][Gene_0220][Gene_0336] #&gt; [Gene_0337][Gene_0670][Gene_0677][Gene_0755][Gene_0778] #&gt; [Gene_0796][Gene_0846][Gene_1001][Gene_1076][Gene_1170] #&gt; [Gene_1230][Gene_1261][Gene_1379][Gene_1821][Gene_1864] #&gt; [Gene_1885][Gene_1934][Gene_0543|Gene_0220:Gene_1001] #&gt; [Gene_1413|Gene_1076][Gene_1482|Gene_0778] #&gt; [Gene_1502|Gene_0048][Gene_1790|Gene_1076] #&gt; [Gene_1840|Gene_0796][Gene_1922|Gene_1001] #&gt; [Gene_0346|Gene_1922] #&gt; nodes: 30 #&gt; arcs: 9 #&gt; undirected arcs: 0 #&gt; directed arcs: 9 #&gt; average markov blanket size: 0.67 #&gt; average neighbourhood size: 0.60 #&gt; average branching factor: 0.30 #&gt; #&gt; learning algorithm: Hill-Climbing #&gt; score: BIC (Gauss.) #&gt; penalization coefficient: 2.649159 #&gt; tests used in the learning procedure: 29 #&gt; optimized: TRUE ## Visualization of glmnet_BIC criteria ## Just to obtain data to be used in the inference gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, returnData=TRUE) set.seed(10) glmnetBICpath(gs$data, sample(colnames(gs$data), 1))[[&quot;plot&quot;]] 3.1.2 MCP_CV and SCAD_CV Same as glmnet_CV, the library performs penalized regression baed on MCP and SCAD using ncvreg library. library(ncvreg) mcp.net &lt;- scstruc(sce, included_genes, algorithm=&quot;MCP_CV&quot;, returnData=FALSE, changeSymbol=FALSE) scad.net &lt;- scstruc(sce, included_genes, algorithm=&quot;SCAD_CV&quot;, returnData=FALSE, changeSymbol=FALSE) ## Using the bnlearn function to compare two networks bnlearn::compare(mcp.net, scad.net) #&gt; $tp #&gt; [1] 7 #&gt; #&gt; $fp #&gt; [1] 0 #&gt; #&gt; $fn #&gt; [1] 2 3.1.3 L0-regularized regression Based on the L0Learn, the package performs structure learning based on L0-regularized regression. 3.1.4 CCDr algorithm Generally, the CCDr algorithm is the fastest algorithm and learns the network for multiple lambdas. By default, the function chooses the network with the best BIC value among multiple lambdas. To supress this effect and obtain all networks, set bestScore to FALSE in the algorithm.args. library(ccdrAlgorithm);library(sparsebnUtils) ccdr.res &lt;- scstruc(sce, included_genes, algorithm=&quot;ccdr&quot;, changeSymbol=FALSE, algorithm.args=list(bestScore=FALSE)) #&gt; Setting `lambdas.length` to 10 #&gt; Returning the bn per lambda from result of ccdr.run names(ccdr.res$net) #&gt; [1] &quot;14.1421356237309&quot; &quot;8.47798757230114&quot; #&gt; [3] &quot;5.08242002399425&quot; &quot;3.04683075789017&quot; #&gt; [5] &quot;1.82652705274248&quot; &quot;1.09497420090059&quot; #&gt; [7] &quot;0.656419787945471&quot; &quot;0.393513324471009&quot; ccdr.res$net[[4]] #&gt; #&gt; Random/Generated Bayesian network #&gt; #&gt; model: #&gt; [Gene_0048][Gene_0165][Gene_0209][Gene_0220][Gene_0336] #&gt; [Gene_0337][Gene_0346][Gene_0543][Gene_0670][Gene_0677] #&gt; [Gene_0755][Gene_0778][Gene_0796][Gene_0846][Gene_1001] #&gt; [Gene_1076][Gene_1170][Gene_1230][Gene_1261][Gene_1379] #&gt; [Gene_1482][Gene_1821][Gene_1840][Gene_1864][Gene_1885] #&gt; [Gene_1934][Gene_1413|Gene_1076][Gene_1502|Gene_0048] #&gt; [Gene_1790|Gene_1076][Gene_1922|Gene_1001] #&gt; nodes: 30 #&gt; arcs: 4 #&gt; undirected arcs: 0 #&gt; directed arcs: 4 #&gt; average markov blanket size: 0.27 #&gt; average neighbourhood size: 0.27 #&gt; average branching factor: 0.13 #&gt; #&gt; generation algorithm: Empty As such, the bootstrap-based inference can be performed very fast using CCDr algorithm. If bootstrapping is specified, the function performs learning the bootstrapped network for the multiple lambdas across all the replicates. 3.1.5 Precision lasso Precision Lasso combines the LASSO and the precision matrix into the regularization process. This approach is particularly useful in biological and genomics studies, where highly-correlated features often appear. We implmented precision lasso in R and the feature is provided by plasso.fit and plasso.fit.single using RcppArmadillo. The fixed lambda value should be specified. pl.res &lt;- scstruc(sce, included_genes, algorithm=&quot;plasso&quot;, changeSymbol=FALSE) References Schmidt, Mark, Alexandru Niculescu-Mizil, and Kevin Murphy. 2007. “Learning Graphical Model Structure Using L1-Regularization Paths.” In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2, 1278–83. AAAI’07. Vancouver, British Columbia, Canada: AAAI Press. "],["hurdle-model.html", "Chapter 4 Hurdle model 4.1 Customized score for hurdle model", " Chapter 4 Hurdle model For handling the zero-inflated nature of the single-cell transcriptomics data, the use of the hurdle model is presented. The hurdle model is a two-part model that models whether the observation is zero or not, and non-zero part separately. Based on the inferred network in HurdleNormal, which proposed the multivariate Hurdle model and grouped lasso to learn the undirected network (McDavid et al. 2019), the directed acyclic graph is inferred based on score maximization using the constraints. The undirected network is selected from multiple lambdas based on the BIC criteria implemented in HurdleNormal. For this function, Hurdle algorithm should be specified in algorithm argument in scstruc. By default, score will be BIC in bnlearn. library(HurdleNormal) library(scstruc) sce &lt;- mockSCE() sce &lt;- logNormCounts(sce) included_genes &lt;- sample(row.names(sce), 20) gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;Hurdle&quot;) gs$net #&gt; #&gt; Bayesian network learned via Score-based methods #&gt; #&gt; model: #&gt; [Gene_0172][Gene_0199][Gene_0307][Gene_0488][Gene_0591] #&gt; [Gene_0730][Gene_0748][Gene_0865][Gene_0958][Gene_1268] #&gt; [Gene_1297][Gene_1377][Gene_1381][Gene_1394][Gene_1416] #&gt; [Gene_1451][Gene_1544][Gene_1558][Gene_1684][Gene_1909] #&gt; nodes: 20 #&gt; arcs: 0 #&gt; undirected arcs: 0 #&gt; directed arcs: 0 #&gt; average markov blanket size: 0.00 #&gt; average neighbourhood size: 0.00 #&gt; average branching factor: 0.00 #&gt; #&gt; learning algorithm: Hill-Climbing #&gt; score: BIC (Gauss.) #&gt; penalization coefficient: 2.649159 #&gt; tests used in the learning procedure: 0 #&gt; optimized: TRUE The score maximization function can be set arbitrarily (maximizeFun), set to hc by default. Greedy Equivalence Search can be performed via setting maximize argument to ges. gs.tabu &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;Hurdle&quot;, algorithm.args=list(maximizeFun=bnlearn::tabu)) gs.tabu$net #&gt; #&gt; Bayesian network learned via Score-based methods #&gt; #&gt; model: #&gt; [Gene_0172][Gene_0199][Gene_0307][Gene_0488][Gene_0591] #&gt; [Gene_0730][Gene_0748][Gene_0865][Gene_0958][Gene_1268] #&gt; [Gene_1297][Gene_1377][Gene_1381][Gene_1394][Gene_1416] #&gt; [Gene_1451][Gene_1544][Gene_1558][Gene_1684][Gene_1909] #&gt; nodes: 20 #&gt; arcs: 0 #&gt; undirected arcs: 0 #&gt; directed arcs: 0 #&gt; average markov blanket size: 0.00 #&gt; average neighbourhood size: 0.00 #&gt; average branching factor: 0.00 #&gt; #&gt; learning algorithm: Tabu Search #&gt; score: BIC (Gauss.) #&gt; penalization coefficient: 2.649159 #&gt; tests used in the learning procedure: 0 #&gt; optimized: TRUE ## This performs GES gs.tabu &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;Hurdle&quot;, algorithm.args=list(maximize=&quot;ges&quot;)) gs.tabu$net #&gt; #&gt; Random/Generated Bayesian network #&gt; #&gt; model: #&gt; [Gene_0172][Gene_0199][Gene_0307][Gene_0488][Gene_0591] #&gt; [Gene_0730][Gene_0748][Gene_0865][Gene_0958][Gene_1268] #&gt; [Gene_1297][Gene_1377][Gene_1381][Gene_1394][Gene_1416] #&gt; [Gene_1451][Gene_1544][Gene_1558][Gene_1684][Gene_1909] #&gt; nodes: 20 #&gt; arcs: 0 #&gt; undirected arcs: 0 #&gt; directed arcs: 0 #&gt; average markov blanket size: 0.00 #&gt; average neighbourhood size: 0.00 #&gt; average branching factor: 0.00 #&gt; #&gt; generation algorithm: Empty 4.1 Customized score for hurdle model Additional score can be specified by using hurdle.bic function in algorithm.args argument. The score is defined as the sum of BIC values from continuous and logistic regression part of the hurdle model. Let \\(Y = [y_{ij}]\\) denote the log-normalized expression value of the gene i in subset cell j and \\(Z = [z_{ij}]\\) a 0-1 indicator value of whether the gene expression is zero. The score is defined as: \\[ \\text{logit}\\left(\\Pr(Z_{ij} = 1)\\right) \\sim \\beta_i G_j, \\] \\[ \\Pr(Y_{ij} = y \\mid Z_{ij} = 1) \\sim N(\\beta_i G_j, \\sigma^2). \\] gs2 &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;Hurdle&quot;, algorithm.args=list(&quot;score&quot;=hurdle.bic)) gs2$net #&gt; #&gt; Bayesian network learned via Score-based methods #&gt; #&gt; model: #&gt; [Gene_0172][Gene_0199][Gene_0307][Gene_0488][Gene_0591] #&gt; [Gene_0730][Gene_0748][Gene_0865][Gene_0958][Gene_1268] #&gt; [Gene_1297][Gene_1377][Gene_1381][Gene_1394][Gene_1416] #&gt; [Gene_1451][Gene_1544][Gene_1558][Gene_1684][Gene_1909] #&gt; nodes: 20 #&gt; arcs: 0 #&gt; undirected arcs: 0 #&gt; directed arcs: 0 #&gt; average markov blanket size: 0.00 #&gt; average neighbourhood size: 0.00 #&gt; average branching factor: 0.00 #&gt; #&gt; learning algorithm: Hill-Climbing #&gt; score: #&gt; User-Provided Score Function #&gt; tests used in the learning procedure: 0 #&gt; optimized: TRUE As proposed in the MAST library, the cellular detection rate adjustment (CDR) can be performed in the scoring phase by cdrAdjuetment to TRUE. This applies inclusion of CDR term in the hurdle modeling and score maximizing phase. References McDavid, Andrew, Raphael Gottardo, Noah Simon, and Mathias Drton. 2019. “GRAPHICAL MODELS FOR ZERO-INFLATED SINGLE CELL GENE EXPRESSION.” The Annals of Applied Statistics 13 (2): 848–73. https://doi.org/10.1214/18-AOAS1213. "],["other-algorithms-and-software.html", "Chapter 5 Other algorithms and software 5.1 Bootstrapping 5.2 PIDC 5.3 GENIE3", " Chapter 5 Other algorithms and software 5.1 Bootstrapping For obtaining the reliable network, we can use bootstrapping approach by randomly subsampling the data, infer the network from it, and average the bootstrapped networks. Most algorithms can use bootstrapping by specifying boot=TRUE. For instance, to use bootstrapping for CCDr algorithm (which is fast), the example codes are shown below. Replication number can be specified by R and sampling number can be specified by m argument. In this way, bn.strength object will be returned as net. library(scstruc) sce &lt;- mockSCE() sce &lt;- logNormCounts(sce) included_genes &lt;- sample(row.names(sce), 40) gs &lt;- scstruc(sce, included_genes, R=30, changeSymbol=FALSE, algorithm=&quot;ccdr&quot;, boot=TRUE) #&gt; Bootstrapping specified #&gt; Returning the list of bn.strength ## It consists of strength from multiple lambdas head(gs$net[[5]]) #&gt; from to strength direction #&gt; 1 Gene_0136 Gene_0278 0 0 #&gt; 2 Gene_0136 Gene_0332 0 0 #&gt; 3 Gene_0136 Gene_0359 0 0 #&gt; 4 Gene_0136 Gene_0444 0 0 #&gt; 5 Gene_0136 Gene_0451 0 0 #&gt; 6 Gene_0136 Gene_0513 0 0 5.2 PIDC PIDC algorithm is based on information theory and leverages partial information decomposition (PID) to analyze and quantify the relationships between multiple variables (Chan, Stumpf, and Babtie 2017). scstruc uses the undirected network produced from PIDC, implemented in Julia. JuliaCall is used to call Julia from R, and the users should first set up the Julia environment. Also, the path to the main library (NetworkInference.jl) should be specified to NetworkInference_HOME argument. The directed acyclic graph is then obtained based on the score maximization with the constraints produced by PIDC. The example code is shown as follows: library(JuliaCall) julia &lt;- julia_setup(JULIA_HOME = &quot;./Julia-1.10.5/bin&quot;) gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;pidc&quot;, algorithm.args=list(NetworkInference_HOME=&quot;./NetworkInference.jl&quot;)) gs$net The bootstrapped-based inference can be also performed. In this case, the averaged network is obtained per thresholds parameters in algorithm.args. By default, the network is thresholded by seq(0.1, 0.4, 0.1). gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;pidc&quot;, boot=TRUE, R=30, algorithm.args=list(NetworkInference_HOME=&quot;./NetworkInference.jl&quot;)) gs$net 5.3 GENIE3 GENIE3 is a widely used method for inferring directed interactions between genes based on an ensemble of regression trees. By specifying genie3 in algorithm, the function performs GENIE3 algorithm and subsequently threshold the arcs. Then, the function returns the list of the original results and the networks if the thresholded network is DAG. gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;genie3&quot;) head(gs$net$original) #&gt; Gene_0136 Gene_0278 Gene_0332 Gene_0359 #&gt; Gene_0136 0.00000000 0.02681822 0.02583674 0.02695983 #&gt; Gene_0278 0.02647994 0.00000000 0.02549723 0.02173386 #&gt; Gene_0332 0.01110906 0.02269165 0.00000000 0.01189988 #&gt; Gene_0359 0.01317634 0.01228216 0.01164556 0.00000000 #&gt; Gene_0444 0.04392987 0.02961976 0.02127639 0.03306372 #&gt; Gene_0451 0.04437585 0.02831226 0.02814917 0.03191775 #&gt; Gene_0444 Gene_0451 Gene_0513 Gene_0617 #&gt; Gene_0136 0.03005392 0.03772602 0.03804853 0.05262988 #&gt; Gene_0278 0.03198088 0.03016941 0.04440042 0.04130562 #&gt; Gene_0332 0.01056729 0.01951618 0.02807687 0.01585391 #&gt; Gene_0359 0.01804888 0.02436020 0.01064142 0.01333352 #&gt; Gene_0444 0.00000000 0.02666756 0.02760363 0.03017295 #&gt; Gene_0451 0.02310180 0.00000000 0.02432412 0.02180153 #&gt; Gene_0761 Gene_0804 Gene_0820 Gene_0945 #&gt; Gene_0136 0.05270084 0.05393383 0.02534319 0.04046751 #&gt; Gene_0278 0.03726217 0.03061855 0.03350037 0.02595288 #&gt; Gene_0332 0.02385015 0.01547912 0.01430316 0.01826825 #&gt; Gene_0359 0.01122995 0.01641431 0.01969875 0.01190332 #&gt; Gene_0444 0.03362310 0.02759881 0.03052395 0.02935651 #&gt; Gene_0451 0.03792805 0.02661168 0.02813276 0.02870738 #&gt; Gene_0962 Gene_0981 Gene_1076 Gene_1085 #&gt; Gene_0136 0.03283452 0.03015960 0.02463039 0.02958978 #&gt; Gene_0278 0.02337200 0.02430229 0.03207479 0.02882045 #&gt; Gene_0332 0.02899968 0.01061925 0.01154685 0.01726142 #&gt; Gene_0359 0.01380798 0.02816001 0.02064544 0.01537234 #&gt; Gene_0444 0.02689631 0.02590470 0.03227473 0.02964127 #&gt; Gene_0451 0.02409904 0.03284650 0.02176073 0.02509007 #&gt; Gene_1128 Gene_1183 Gene_1254 Gene_1298 #&gt; Gene_0136 0.02701037 0.02528709 0.03143063 0.02685596 #&gt; Gene_0278 0.03535969 0.02710109 0.02627006 0.02953439 #&gt; Gene_0332 0.02288763 0.01719658 0.01438780 0.02263469 #&gt; Gene_0359 0.01855230 0.01543778 0.01565215 0.02355914 #&gt; Gene_0444 0.02642368 0.03239411 0.03335679 0.02986710 #&gt; Gene_0451 0.02780313 0.02764364 0.03803860 0.02767374 #&gt; Gene_1343 Gene_1394 Gene_1452 Gene_1490 #&gt; Gene_0136 0.02523095 0.03806352 0.02691441 0.02594802 #&gt; Gene_0278 0.02302105 0.06999887 0.03031623 0.02685602 #&gt; Gene_0332 0.01734836 0.01016982 0.01185967 0.01084433 #&gt; Gene_0359 0.01706814 0.01247615 0.01290060 0.01390558 #&gt; Gene_0444 0.02933161 0.04557776 0.02740260 0.03293274 #&gt; Gene_0451 0.02048293 0.01685062 0.02151410 0.03685426 #&gt; Gene_1520 Gene_1550 Gene_1628 Gene_1653 #&gt; Gene_0136 0.03090229 0.03220466 0.02676684 0.02895982 #&gt; Gene_0278 0.03402692 0.02964903 0.03309798 0.02739125 #&gt; Gene_0332 0.01370553 0.02019554 0.01342644 0.01393480 #&gt; Gene_0359 0.01483200 0.01373625 0.02382146 0.01344619 #&gt; Gene_0444 0.02745620 0.03690006 0.03028275 0.03233850 #&gt; Gene_0451 0.03052870 0.02719006 0.02460855 0.02492003 #&gt; Gene_1668 Gene_1729 Gene_1743 Gene_1789 #&gt; Gene_0136 0.03114647 0.02670969 0.02267142 0.02489048 #&gt; Gene_0278 0.02575499 0.02500280 0.02420977 0.02557378 #&gt; Gene_0332 0.01814787 0.02886645 0.03012805 0.01355017 #&gt; Gene_0359 0.01813936 0.02244381 0.01001618 0.01330474 #&gt; Gene_0444 0.05009842 0.03149565 0.03212288 0.03268932 #&gt; Gene_0451 0.02463575 0.03253546 0.01925182 0.03050675 #&gt; Gene_1854 Gene_1859 Gene_1870 Gene_1896 #&gt; Gene_0136 0.03210645 0.03411090 0.04478465 0.02680080 #&gt; Gene_0278 0.02749634 0.02999234 0.03662405 0.04064160 #&gt; Gene_0332 0.02087293 0.02086527 0.01344448 0.01463337 #&gt; Gene_0359 0.01457553 0.02090480 0.01431742 0.01363071 #&gt; Gene_0444 0.02914333 0.04321472 0.03010043 0.03566941 #&gt; Gene_0451 0.04010635 0.02395841 0.02702266 0.02222910 #&gt; Gene_1912 Gene_1965 Gene_1967 Gene_1989 #&gt; Gene_0136 0.02700852 0.02821939 0.03549639 0.02821415 #&gt; Gene_0278 0.02656766 0.03111966 0.02828119 0.03044887 #&gt; Gene_0332 0.01166855 0.01448439 0.02107201 0.02435457 #&gt; Gene_0359 0.01644794 0.01658699 0.01974518 0.01389435 #&gt; Gene_0444 0.03025733 0.02698903 0.03273914 0.03150617 #&gt; Gene_0451 0.02568175 0.02391937 0.02168935 0.03111619 References Chan, Thalia E., Michael P. H. Stumpf, and Ann C. Babtie. 2017. “Gene Regulatory Network Inference from Single-Cell Data Using Multivariate Information Measures.” Cell Systems 5 (3): 251–267.e3. https://doi.org/10.1016/j.cels.2017.08.014. "],["evaluating-the-inferred-networks.html", "Chapter 6 Evaluating the inferred networks 6.1 Evaluation functions 6.2 Evaluating the causal validity", " Chapter 6 Evaluating the inferred networks For the interpretation of the results, the assessment of inferred networks is crucial. One of the core features of the scstruc is evaluating and selecting optimal algorithms from the inferred networks. We describe how to evaluate the inferred networks using various metrics in this section. The implemented metrics include: True positive arcs False positive arcs False negative arcs Precision Recall F1-score Bayesian Information Criterion (if the data to be fitted is provided) Structural Hamming Distance Structural Intervention Distance (with or without symmetrization) Kullback–Leibler divergence 6.1 Evaluation functions 6.1.1 metrics function This function accepts learned networks and the reference network (both should be bn object) and outputs data.frame consisting of various metrics. library(scstruc) net &lt;- readRDS(&quot;ecoli70.rds&quot;) data.inference &lt;- rbn(net, 50) infer &lt;- hc(data.inference) metrics(bn.net(net), list(&quot;inferred&quot;=infer)) #&gt; algo referenceNode InferenceNode s0 edges SHD TP FP #&gt; 1 inferred 46 46 70 103 108 28 42 #&gt; FN TPR Precision Recall F1 SID KL BIC #&gt; 1 75 0.4 0.4 0.2718447 0.3236994 999 NA NA sid_sym argument can choose whether to symmetrze the SID, and SID.cran can choose whether to use SID implemented in CRAN package SID. 6.1.2 metricsFromFitted function This function accepts parameter-fitted network and sampling number, as well as the algorithms to be used in the inference. Using rbn function in bnlearn, logic sampling is performed from fitted network. Here, we use ECOLI70 network from GeneNet R package, sampling 50 observations from the network. The testing algorithms can be specifed by argument algos. For the special algorithms, the arguments with the same name are provided. The function has return_data and return_net argument, which returns the data used in the inference and inferred networks. By default, only the metrics is returned. mf &lt;- metricsFromFitted(net, 50, algos=c(&quot;glmnet_CV&quot;, &quot;glmnet_BIC&quot;, &quot;L0_CV&quot;)) #&gt; glmnet_CV 16.1115391254425 #&gt; glmnet_BIC 1.22309803962708 #&gt; L0_CV #&gt; 16.5114049911499 #&gt; MMHC 0.001 0.0976150035858154 #&gt; MMHC 0.005 0.0993959903717041 #&gt; MMHC 0.01 0.115104913711548 #&gt; MMHC 0.05 0.162407159805298 #&gt; Network computing finished head(mf$metrics) #&gt; algo s0 edges KL BIC SHD TP FP FN #&gt; 1 glmnet_CV 70 61 6.403812 -2331.570 66 27 43 34 #&gt; 2 glmnet_BIC 70 40 23.641317 -2484.137 57 22 48 18 #&gt; 3 L0_CV 70 22 70.194805 -2979.877 64 10 60 12 #&gt; 4 mmhc_0.001 70 27 47.243168 -2793.869 59 15 55 12 #&gt; 5 mmhc_0.005 70 30 43.927981 -2720.571 56 16 54 14 #&gt; 6 mmhc_0.01 70 31 39.815723 -2673.261 56 18 52 13 #&gt; TPR Precision Recall F1 SID PPI #&gt; 1 0.3857143 0.3857143 0.4426230 0.4122137 NA NA #&gt; 2 0.3142857 0.3142857 0.5500000 0.4000000 NA NA #&gt; 3 0.1428571 0.1428571 0.4545455 0.2173913 NA NA #&gt; 4 0.2142857 0.2142857 0.5555556 0.3092784 NA NA #&gt; 5 0.2285714 0.2285714 0.5333333 0.3200000 NA NA #&gt; 6 0.2571429 0.2571429 0.5806452 0.3564356 NA NA #&gt; time BICnorm N p #&gt; 1 16.11153913 0.9726594 50 46 #&gt; 2 1.22309804 0.8764378 50 46 #&gt; 3 16.51140499 0.5637837 50 46 #&gt; 4 0.09761500 0.6810957 50 46 #&gt; 5 0.09939599 0.7273233 50 46 #&gt; 6 0.11510491 0.7571609 50 46 The results can be visualized in the usual way by using the library like ggplot2. We use here plotthis library for visualizing. library(plotthis) library(ggplot2) library(ggrepel) ScatterPlot(mf$metrics, x=&quot;SHD&quot;, y=&quot;F1&quot;, color_by=&quot;algo&quot;, legend.position=&quot;none&quot;) + geom_text_repel(aes(label=algo), bg.colour=&quot;white&quot;) 6.2 Evaluating the causal validity The primary objective of the package is evaluating the causal validity of the inferred networks. Two approach can be used, in the situations that the reference directed network is available or not. In most of the cases, the reference networks is not readily available. 6.2.1 Obtaining the directed acyclic graphs (DAGs) for the evaluation For the interesting biological pathway, one can obtain DAG from the KEGG PATHWAY. The getKEGGEdges function accepts the pathway identifier and returns the DAG, though it will not always succeed. The function first parses the pathway information using ggkegg, and identify the largest components to be evaluated. If removeCycle is TRUE, the function identifies the minimum feedback using igraph function and remove these edges. This returns the bn object. Suppose you are interested in inferring gene regulatory networks in mTOR signaling pathway from your dataset, you should first load DAG from the KEGG API. library(scstruc) dags &lt;- getKEGGEdges(&quot;mmu04150&quot;, removeCycle=TRUE) #&gt; Removing Pik3ca|Mtor graphviz.plot(dags) Using the genes in this candidate pathway, inference is performed and performance metrics can be obtained based on the reference DAG. mymet &lt;- metrics(dags, list(&quot;Algo1&quot;=net)) 6.2.2 Intersection-Validation approach In case there are no reference networks, we can use Insersection-Validation approach, proposed by Viinikka et al. (Viinikka, Eggeling, and Koivisto 2018) to evaluate which algorithm is optimal in terms of SHD and SID. For this purpose, interVal function is prepared. The function accepts input data, multiple algorithms to be tested, and parameters related to Intersection-Validation. The user should provide data, algorithms to be tested, It leaves a message if connected node pairs, defined as the number of edges in the agreement graph, is below 15. References Viinikka, Jussi, Ralf Eggeling, and Mikko Koivisto. 2018. “Intersection-Validation: A Method for Evaluating Structure Learning Without Ground Truth.” In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, 1570–78. PMLR. https://proceedings.mlr.press/v84/viinikka18a.html. "],["plot.html", "Chapter 7 Plotting functions 7.1 plotNet 7.2 plotAVN 7.3 ggraph.compare 7.4 plotSubNet", " Chapter 7 Plotting functions The library has various functions for visualizing the inferred networks with highlighting the important regulatory relationships and comparing the inferred networks by multiple algorithms, using the ggraph, tidygraph and ggfx. These can be useful for the diagnosis and for the publication figure. Mostly, the function accepts the bn object from bnlearn. 7.1 plotNet plotNet function accepts bn object. library(scstruc) sce &lt;- mockSCE() sce &lt;- logNormCounts(sce) included_genes &lt;- sample(row.names(sce), 50) gs &lt;- scstruc(sce, included_genes, changeSymbol=FALSE, algorithm=&quot;glmnet_BIC&quot;, returnData=TRUE) ## Vanilla plot plotNet(gs$net) Optionally, data can be passed to fit the parameters and the function colors the edge using the fitted parameters. The nodes will be sized by their degrees. degreeMode argument can be specified for how the degree will be calculated. 7.2 plotAVN This function is intended specifically for visualizing the bootstrapped networks. You can pass the results of functions returning bn.strength object, to this function. The threshold will be automatically determined if not specified. In the plot, the strength are shown as edge width, and the direction are shown as edge color. The node sizes will be determined by the degree. # plotAVN() Edges can be highlighted by specifying highlightEdges argument, as same as plotNet. In the plotAVN, the with_outer_glow function in ggfx package is used to highlight edges as to preserve the strength and direction mapping in the plot. 7.3 ggraph.compare Similar to graphviz.compare function in bnlearn, the diagnostic plot can be made by ggraph.compare. This accepts the list of bn object and plot the nodes and each edge associated with the graphs by geom_edge_parallel. The agreement of edges in multiple graphs can be understood with the function, however, it will get too complicated if many graphs are provided. 7.4 plotSubNet plotSubNet plots the network centered to the interesting genes. "],["actual.html", "Chapter 8 Analysis example of publicly available SCT data", " Chapter 8 Analysis example of publicly available SCT data Here, we show analytic example of actual SCT data using scstruc. We will demonstrate how to infer GRN and evaluate them using the library. First, we need to load the data into SingleCellExperiment (SCE) object. This part is time consuming and we read RDS file storing the list of SCE objects from each sample. "],["references.html", "References", " References Aragam, Bryon, and Qing Zhou. 2015. “Concave Penalized Estimation of Sparse Gaussian Bayesian Networks.” Journal of Machine Learning Research 16 (69): 2273–2328. http://jmlr.org/papers/v16/aragam15a.html. Bilous, Mariia, Loc Tran, Chiara Cianciaruso, Aurélie Gabriel, Hugo Michel, Santiago J Carmona, Mikael J Pittet, and David Gfeller. 2022. “Metacells Untangle Large and Complex Single-Cell Transcriptome Networks.” BMC Bioinformatics 23 (1): 336. https://doi.org/10.1186/s12859-022-04861-1. Chan, Thalia E., Michael P. H. Stumpf, and Ann C. Babtie. 2017. “Gene Regulatory Network Inference from Single-Cell Data Using Multivariate Information Measures.” Cell Systems 5 (3): 251–267.e3. https://doi.org/10.1016/j.cels.2017.08.014. McDavid, Andrew, Raphael Gottardo, Noah Simon, and Mathias Drton. 2019. “GRAPHICAL MODELS FOR ZERO-INFLATED SINGLE CELL GENE EXPRESSION.” The Annals of Applied Statistics 13 (2): 848–73. https://doi.org/10.1214/18-AOAS1213. Sato, Noriaki, Yoshinori Tamada, Guangchuang Yu, and Yasushi Okuno. 2022. “CBNplot: Bayesian Network Plots for Enrichment Analysis.” Bioinformatics (Oxford, England) 38 (10): 2959–60. https://doi.org/10.1093/bioinformatics/btac175. Schmidt, Mark, Alexandru Niculescu-Mizil, and Kevin Murphy. 2007. “Learning Graphical Model Structure Using L1-Regularization Paths.” In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2, 1278–83. AAAI’07. Vancouver, British Columbia, Canada: AAAI Press. Viinikka, Jussi, Ralf Eggeling, and Mikko Koivisto. 2018. “Intersection-Validation: A Method for Evaluating Structure Learning Without Ground Truth.” In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, 1570–78. PMLR. https://proceedings.mlr.press/v84/viinikka18a.html. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
