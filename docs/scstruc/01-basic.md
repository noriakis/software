

# Basic usage of `scstruc`

This part will present how to use the package for the inference of the network and evaluation. Basically, if you have the object storing (normalized) expression matrix, the primary function `scstruc` can perform inference. We will use the mock data generated by `scran::mockSCE` function here.


``` r
library(scran)
library(scstruc)
library(ggraph)
library(bnlearn)
library(tibble)
set.seed(0)
sce <- mockSCE()
sce <- logNormCounts(sce)
sce
#> class: SingleCellExperiment 
#> dim: 2000 200 
#> metadata(0):
#> assays(2): counts logcounts
#> rownames(2000): Gene_0001 Gene_0002 ... Gene_1999
#>   Gene_2000
#> rowData names(0):
#> colnames(200): Cell_001 Cell_002 ... Cell_199
#>   Cell_200
#> colData names(4): Mutation_Status Cell_Cycle
#>   Treatment sizeFactor
#> reducedDimNames(0):
#> mainExpName: NULL
#> altExpNames(1): Spikes
```
We obtain simulated data by `mockSCE` function in scuttle, and log-normalize the count with the default parameter.

## Bayesian network

Bayesian networks are probabilistic graphical models that represent a set of random variables $X = \{X_1, \dots, X_N\}$, and their conditional dependencies using a directed acyclic graph (DAG). They are widely used in various domains, such as economics, medicine, decision-making, and for modeling complex relationships among variables such as transcriptomics data. In the package, we infer the relationships among genes, representing the transcriptional regulation between them.

The graphical separation between nodes in G implies the conditional independence of the corresponding variables and leads to factorization:

$$
P(X \mid G; \theta) = \prod_{i=1}^{N} P(X_i \mid \pi_{X_i}; \theta_{X_i}) \quad \text{where } \pi_{X_i} = \{ \text{parents of } X_i \text{ in } G \}
$$

In the package, various algorithms are implemented tailored to account for the nature of SCT data, such as dropouts.


## Preprocessing of single-cell transcriptomics data and preparation

To reduce computational burden and to obtain interpretable results, some preprocessing steps and the preparation is suggested.

### Coarse-graining expression data (metacell expression)

The package can use metacell approach to reduce the computational burden required to perform structure learning from SCT data. The package features `superCellMat` function using `SuperCell` package available in CRAN. It employs a cell aggregation approach to reduce dataset complexity while preserving key biological variabilityã€€[@bilous_metacells_2022]. This step is optional and one can use their untransformed data directly.


``` r
library(SuperCell)
dim(sce)
#> [1] 2000  200
rowData(sce)["ID"] <- row.names(sce)
sce <- superCellMat(sce, ID="ID")
#>    2000 20
dim(sce)
#> [1] 2000   20
```

### Obtaining interesting gene identifiers

For inference, we first subset to genes in the interesting biological pathway (such as ECM receptor interaction in the cells annotated as vascular endothelial cells). For this purpose, two functions are prepared. One is fetching gene identifiers from Gene Ontology, and another is from KEGG PATHWAY.

#### `getGOGenes`

We can obtain genes involved in gene ontology by using the identifier.


``` r
library(scstruc)
library(org.Mm.eg.db)
genes <- getGOGenes("GO:0030198", orgDb=org.Mm.eg.db)
genes[1:5]
#> [1] "Abl1"    "Abl1"    "Abl2"    "Adamts1" "Adamts1"
```

#### `getKEGGPathwayGenes`

We can obtain genes involved in KEGG PATHWAY by using the KEGG identifier with organism identifier. The below is obtaining genes involved in mTOR signaling pathway in _Mus musculus_. You need to specify organism database in order to correcrly convert the obtained identifiers.


``` r
library(KEGGREST)
genes <- getKEGGPathwayGenes("mmu04150", orgDb=org.Mm.eg.db)
genes[1:5]
#> [1] "Prkaa1"  "Prkaa2"  "Atp6v1h" "Prr5"    "Braf"
```

You can pass these genes for the inference by `scstruc` function. Also, we published a package that estimates GRNs from omics data based on enrichment analysis results in the past, and the approach could be useful [@sato_cbnplot_2022].

## Actual structure learning

`scstruc` function needs `SingleCellExperiment` object or those storing gene expression data. Also, gene list must be specified. In this analysis, randomly subset gene identifiers are used as input. `changeSymbol` argument here refers to the changing of the gene names of subset matrix to those specified in `symbolColumn` argument (default to `Symbol`). In this example, we do not have corresponding symbol and the argument is set to FALSE. Also, algorithm is set to `mmhc` by default, which performs max-min hill climbing approach.


``` r
included_genes <- sample(row.names(sce), 100)
gs <- scstruc(sce, included_genes, changeSymbol=FALSE)
names(gs)
#> [1] "net"  "data"
gs$net
#> 
#>   Bayesian network learned via Hybrid methods
#> 
#>   model:
#>    [Gene_0019][Gene_0061][Gene_0097][Gene_0099][Gene_0101]
#>    [Gene_0108][Gene_0123][Gene_0198][Gene_0224][Gene_0239]
#>    [Gene_0242][Gene_0290][Gene_0292][Gene_0322][Gene_0325]
#>    [Gene_0332][Gene_0352][Gene_0354][Gene_0374][Gene_0418]
#>    [Gene_0433][Gene_0451][Gene_0630][Gene_0642][Gene_0676]
#>    [Gene_0744][Gene_0750][Gene_0795][Gene_0831][Gene_0900]
#>    [Gene_1018][Gene_1024][Gene_1030][Gene_1048][Gene_1124]
#>    [Gene_1144][Gene_1178][Gene_1180][Gene_1241][Gene_1273]
#>    [Gene_1283][Gene_1316][Gene_1342][Gene_1512][Gene_1646]
#>    [Gene_1672][Gene_1795][Gene_1889][Gene_1976]
#>    [Gene_0238|Gene_0019:Gene_0795][Gene_0466|Gene_0224]
#>    [Gene_0558|Gene_0322][Gene_0580|Gene_0332]
#>    [Gene_0711|Gene_0433:Gene_0795:Gene_1273]
#>    [Gene_0716|Gene_0676][Gene_0772|Gene_0354]
#>    [Gene_0840|Gene_0642][Gene_0859|Gene_0744]
#>    [Gene_0954|Gene_0642][Gene_0979|Gene_0325]
#>    [Gene_0980|Gene_0433][Gene_1059|Gene_0290]
#>    [Gene_1084|Gene_0101][Gene_1132|Gene_1030]
#>    [Gene_1173|Gene_0097:Gene_1512][Gene_1310|Gene_0292]
#>    [Gene_1360|Gene_1024][Gene_1372|Gene_1048]
#>    [Gene_1384|Gene_0433][Gene_1483|Gene_1018:Gene_1273]
#>    [Gene_1487|Gene_1030:Gene_1672][Gene_1509|Gene_0292]
#>    [Gene_1542|Gene_1342][Gene_1642|Gene_1178:Gene_1241]
#>    [Gene_1677|Gene_1180:Gene_1889][Gene_1739|Gene_0676]
#>    [Gene_1751|Gene_1646][Gene_1775|Gene_1316]
#>    [Gene_1855|Gene_0108][Gene_1859|Gene_0831:Gene_1144]
#>    [Gene_1863|Gene_0630][Gene_1982|Gene_0061]
#>    [Gene_0427|Gene_0108:Gene_1059][Gene_0792|Gene_1360]
#>    [Gene_1257|Gene_1863][Gene_1262|Gene_0900:Gene_1855]
#>    [Gene_1518|Gene_0451:Gene_1982][Gene_1519|Gene_1084]
#>    [Gene_1653|Gene_0772][Gene_1804|Gene_1775]
#>    [Gene_0048|Gene_0198:Gene_1804][Gene_0617|Gene_1257]
#>    [Gene_0786|Gene_0427][Gene_1159|Gene_0792]
#>    [Gene_1742|Gene_0792][Gene_0398|Gene_0786]
#>    [Gene_0463|Gene_0786:Gene_1342]
#>    [Gene_0490|Gene_0352:Gene_1159:Gene_1283]
#>    [Gene_1140|Gene_1742][Gene_1991|Gene_1140]
#>   nodes:                                 100 
#>   arcs:                                  67 
#>     undirected arcs:                     0 
#>     directed arcs:                       67 
#>   average markov blanket size:           1.70 
#>   average neighbourhood size:            1.34 
#>   average branching factor:              0.67 
#> 
#>   learning algorithm:                    
#>                                         Max-Min Hill-Climbing 
#>   constraint-based method:               
#>                                       Max-Min Parent Children 
#>   conditional independence test:         
#>                                         Pearson's Correlation 
#>   score-based method:                    Hill-Climbing 
#>   score:                                 BIC (Gauss.) 
#>   alpha threshold:                       0.05 
#>   penalization coefficient:              1.497866 
#>   tests used in the learning procedure:  22042 
#>   optimized:                             TRUE
```

By default, it returns the `bn` object from `bnlearn` and the actual data used for the inference. If `returnBn` is set to `FALSE`, the function returns the `tidygraph` object. These can be used for various downstream tasks, such as visualization, probabilistic reasoning, evaluation based on biological pathway information. The details including the use of various algorithms for learning is described in the following sections.
